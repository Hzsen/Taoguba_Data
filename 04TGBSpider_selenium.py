from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time
import random
import datetime
import MySqlConnect
from webdriver_manager.chrome import ChromeDriverManager


def create_driver():
    options = Options()
    options.add_argument("--start-maximized")

    # âœ… ä½¿ç”¨ä¸€ä¸ªå…¨æ–°çš„ç‹¬ç«‹ç”¨æˆ·æ•°æ®ç›®å½•ï¼Œé¿å… Chrome æ‹’ç»è®¿é—®
    options.add_argument("--user-data-dir=" + "/Users/hzs/selenium-chrome-profile")

    # å¯é€‰ï¼šæŒ‡å®š profile åï¼ˆDefault æˆ–æ–°å»ºçš„ï¼‰
    options.add_argument("--profile-directory=Default")

    # âœ… åˆå§‹åŒ– ChromeDriver
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    return driver




# ========== æ•°æ®åº“å…¥åº“ ==========
def save2DB_blog(blog_id, title, pub_time, content, views, comments):
    sql = f"""
    INSERT INTO tgb_blog (
        id, title, time, content, views, comments
    ) VALUES (
        "{blog_id}", "{title}", "{pub_time}", "{content}", "{views}", "{comments}"
    )
    """
    MySqlConnect.edit(sql)

def save2DB_comment(postid, replyid, comment):
    sql = f"""
    INSERT INTO tgb_comment (
        postid, replyid, comment
    ) VALUES (
        "{postid}", "{replyid}", "{comment}"
    )
    """
    MySqlConnect.edit(sql)

# ========== è§£æåšå®¢æ­£æ–‡ä¸è¯„è®º ==========
def parse_blog(driver, blog):
    try:
        driver.get(blog["url"])
        time.sleep(2 + random.random())

        soup = BeautifulSoup(driver.page_source, "html.parser")

        content_tag = soup.select_one(".blog-content")
        content = content_tag.text.strip() if content_tag else ""

        time_tag = soup.select_one(".blog-date")
        pub_time = time_tag.text.strip() if time_tag else datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        views = int(soup.select_one(".read-num").text.strip()) if soup.select_one(".read-num") else 0
        comments = int(soup.select_one(".comment-num").text.strip()) if soup.select_one(".comment-num") else 0

        title_tag = soup.select_one(".blog-title")
        title = title_tag.text.strip() if title_tag else blog["title"]

        save2DB_blog(blog["id"], title, pub_time, content, views, comments)

        comment_tags = soup.select(".comment-list .comment-item")
        reply_id = 1
        for tag in comment_tags:
            comment_text = tag.text.strip()
            save2DB_comment(blog["id"], reply_id, comment_text)
            reply_id += 1

        print(f"âœ… æŠ“å–æˆåŠŸï¼š{blog['id']} - {title}")
        # driver.save_screenshot(f"screenshot_{blog['id']}.png")  # å¯é€‰æˆªå›¾ä¿å­˜

    except Exception as e:
        print(f"[âŒ æŠ“å–å¤±è´¥] {blog['url']} -> {e}")

# ========== ä¸»æµç¨‹ ==========
def run():
    driver = create_driver()
    print("ğŸ” è¯·åœ¨æ‰“å¼€çš„æµè§ˆå™¨çª—å£ä¸­å®Œæˆç™»å½•åæŒ‰å›è½¦...")
    driver.get("https://www.tgb.cn/login")
    input("âœ… ç™»å½•å®Œæˆåè¯·æŒ‰å›è½¦ç»§ç»­...")

    with open("blog_links.txt", "r", encoding="utf-8") as f:
        links = [line.strip() for line in f if line.strip()]

    for link in links:
        print(f"ğŸ” æ­£åœ¨æŠ“å–ï¼š{link}")
        blog_id = link.split("/")[-1]
        blog = {"id": blog_id, "title": "N/A", "url": link}
        parse_blog(driver, blog)
        time.sleep(random.uniform(1, 2))

    driver.quit()
    print("âœ… æ‰€æœ‰é“¾æ¥æŠ“å–å®Œæ¯•ï¼")

if __name__ == "__main__":
    run()
